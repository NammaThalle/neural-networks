{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Packages\n",
    "Import all the packages required.\n",
    "1. __numpy__ - required for scientific computing with Python.\n",
    "2. __h5py__ - to retrieve dataset stored in h5 file.\n",
    "3. __matplotlib__ - to plot graphs in Python.\n",
    "4. __copy__ - to use deepcopy - making a totally different object\n",
    "5. __scipy__ - used to test with own picture at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import scipy\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Loading the dataset of cat/non-cat picture\n",
    "_Note This dataset is taken from [Kaggle][1] website_\n",
    "\n",
    "[1]: https://www.kaggle.com/baners/a-logistic-regression-classifier-to-recognize-cats/data\n",
    "\n",
    "1. Load the training dataset and extract the :\n",
    "   * x_train - pixel values for each image for training the model\n",
    "   * y_train - corresponding cat/non-cat value for training the model\n",
    "2. Load the testing dataset and extract the :\n",
    "   * x_test - pixel values for each image for testing the model\n",
    "   * y_test - corresponding cat/non-cat value for testing the model's output\n",
    "3. Extract the classes. (In our case \"cat\" and \"non-cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    # load the training dataset\n",
    "    train_dataset = h5py.File('dataset/train_catvnoncat.h5', \"r\")\n",
    "    print(\"Structure of the train dataset : \"+ str(train_dataset.keys()))\n",
    "    x_train_original = np.array(train_dataset['train_set_x'])\n",
    "    y_train_original = np.array(train_dataset['train_set_y'])\n",
    "    \n",
    "    # load the testing dataset\n",
    "    test_dataset = h5py.File('dataset/test_catvnoncat.h5', \"r\")\n",
    "    print(\"Structure of the test dataset : \"+ str(test_dataset.keys()))\n",
    "    x_test_original = np.array(test_dataset['test_set_x'])\n",
    "    y_test_original = np.array(test_dataset['test_set_y'])\n",
    "\n",
    "    # load the classification classes\n",
    "    classes = np.array(test_dataset['list_classes'])\n",
    "\n",
    "    return x_train_original, y_train_original, x_test_original, y_test_original, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Preprocessing the data\n",
    "1. Figure out the dimensions and the shape of the data \n",
    "   * m_train - number of training examples\n",
    "   * m_test - number of testing examples\n",
    "   * num_px - height and width of images (in our case height = width)\n",
    "2. Flatten the image data of 3 RGB channels (numpx, numpx, numpx) into single vectors of shape (numpx * num_px * 3, 1)\n",
    "3. Standardize (Normalize) the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(x_train_original, y_train_original, x_test_original, y_test_original):\n",
    "\n",
    "    # getting dimensions and shape of the data\n",
    "    m_train = x_train_original.shape[0]\n",
    "    m_test = x_test_original.shape[0]\n",
    "    num_px = x_test_original.shape[1]\n",
    "\n",
    "    # change the shape of the labels from (m,) to (1,m)\n",
    "    y_train_preprocess = y_train_original.reshape(1, y_train_original.shape[0])\n",
    "    y_test_preprocess = y_test_original.reshape(1, y_test_original.shape[0])\n",
    "\n",
    "    print(\"Number of testing examples = \" + str(m_train))\n",
    "    print(\"Number of testing examples = \" + str(m_test))\n",
    "    print(\"Height/Width of each image = \" + str(num_px))\n",
    "\n",
    "    print(\"\\nOriginal training data shape: \" + str(x_train_original.shape))\n",
    "    print(\"Original testing data shape: \" + str(x_test_original.shape))\n",
    "\n",
    "    # flatten the data\n",
    "    x_train_flatten = x_train_original.reshape(m_train, -1).T\n",
    "    x_test_flatten = x_test_original.reshape(m_test, -1).T\n",
    "\n",
    "    print(\"\\nFlattened training data shape: \" + str(x_train_flatten.shape))\n",
    "    print(\"Flattened testing data shape: \" + str(x_test_flatten.shape))\n",
    "\n",
    "    # standardize the data\n",
    "    x_train_standard = x_train_flatten / 255\n",
    "    x_test_standard = x_test_flatten / 255\n",
    "\n",
    "    return x_train_standard, y_train_preprocess, x_test_standard, y_test_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Model Training\n",
    "Model Training involves many steps\n",
    "1. Initialize the weights and the bias to __zero__ value\n",
    "2. Compute the sigmoid function. $sigmoid(z) = \\frac{1}{1 + e^{-z}}$ for $z = w^T x + b$\n",
    "    * for weights (w) and bias (b)\n",
    "3. Forward Propagation of the network\n",
    "   1. Get x_train\n",
    "   2. Calculate Activation function: $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "   3. Calculate the Cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)}))$\n",
    "4. Backward Propagation of the network\n",
    "   1. Find the gradients using the formulas:\n",
    "   $$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n",
    "   $$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$\n",
    "5. Optimize the network using the __gradient descent__\n",
    "   1. Learn the $w$ and $b$ by minimizing the cost function $J$\n",
    "   2. Update rule for optimizing the parameters is $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, where $\\alpha$ is the learning rate\n",
    "6. Build a predictor for predicting outcomes\n",
    "   1. Find the Activation for prediction data set (x_pred) using formula in step 3.2\n",
    "   2. if Activation > 0.5 -> cat picture (y_pred = 1)\n",
    "7. Create a Model using steps 1-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the parameters to zero\n",
    "def initialize_to_zero(dim):\n",
    "\n",
    "    w = np.zeros((dim, 1))\n",
    "    b = float(0)\n",
    "\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the sigmoid\n",
    "def sigmoid(z):\n",
    "\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propagate through the network\n",
    "def propagate(w, b, X, Y):\n",
    "\n",
    "    # Find the number of examples in the given dataset X\n",
    "    m = X.shape[1]\n",
    "    # Forward Propagation of the network\n",
    "    ## ------------------------------------------------------------------------ ##\n",
    "    \n",
    "    ## Calculate activation\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "\n",
    "    ## Calculate the cost \n",
    "    cost = (1 / m) * np.sum((Y * np.log(A)) + ((1 - Y) * np.log(1 - A)))\n",
    "    ## ------------------------------------------------------------------------ ##\n",
    "\n",
    "    # Backward Propagation of the network\n",
    "    ## ------------------------------------------------------------------------ ##\n",
    "\n",
    "    dz = A - Y\n",
    "\n",
    "    ## Calculate the gradients\n",
    "    dw = (1 / m) * np.dot(X, dz.T)\n",
    "    db = (1 / m) * np.sum(dz)\n",
    "    ## ------------------------------------------------------------------------ ##\n",
    "\n",
    "    # prepare the return variables\n",
    "    cost = np.squeeze(np.array(cost))\n",
    "    gradients = {\"dw\": dw, \"db\": db}\n",
    "\n",
    "    return gradients, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the network\n",
    "def optimize(w, b, X, Y, num_iterations = 2000, learning_rate = 0.05, print_cost_requested = False):\n",
    "\n",
    "    w = copy.deepcopy(w)\n",
    "    b = copy.deepcopy(b)\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        gradients, cost = propagate(w, b, X, Y)\n",
    "\n",
    "        dw = gradients[\"dw\"]\n",
    "        db = gradients[\"db\"]\n",
    "\n",
    "        # update the learning parameters\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "\n",
    "        if print_cost_requested and i % 100 == 0:\n",
    "            print(\"Cost after iteration %i = %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w, \"b\": b}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a predictor for predicting outcomes\n",
    "def predict(w, b, x_prediction):\n",
    "    \n",
    "    m = x_prediction.shape[1]\n",
    "    y_prediction = np.zeros((1, m))\n",
    "    w = w.reshape(x_prediction.shape[0], 1)\n",
    "\n",
    "    A = sigmoid(np.dot(w.T, x_prediction) + b)\n",
    "\n",
    "    for i in range(A.shape[1]):\n",
    "        if A[0, i] > 0.5:\n",
    "            y_prediction[0, i] = 1\n",
    "        else:\n",
    "            y_prediction[0, i] = 0\n",
    "    \n",
    "    return y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model using above functions\n",
    "def model(x_train, y_train, x_test, y_test, num_iterations = 2000, learning_rate = 0.05, print_cost_requested = False):\n",
    "\n",
    "    # initialize the parameters w and b\n",
    "    w, b = initialize_to_zero(x_train.shape[0])\n",
    "\n",
    "    # Traverse the pipeline and train the model\n",
    "    # Forward Propagation - calculate the activation and cost\n",
    "    # Backward Propagation - calculate the gradients\n",
    "    # Optimization - optimize the cost using gradient descent\n",
    "    print(\"Model Training\")\n",
    "    params = optimize(w, b, x_train, y_train, num_iterations, learning_rate, print_cost_requested)\n",
    "    print(\"Model Training completed\")\n",
    "    \n",
    "    # Perform the Accuracy check and Testing using the Testing data \n",
    "    w = params[\"w\"]\n",
    "    b = params[\"b\"]\n",
    "\n",
    "    y_train_prediction = predict(w, b, x_train)\n",
    "    print(\"Training Accuracy = {} %\".format(100 - np.mean(np.abs(y_train_prediction - y_train)) * 100))\n",
    "\n",
    "    y_test_prediction = predict(w, b, x_test)\n",
    "    print(\"Testing Accuracy = {} %\".format(100 - np.mean(np.abs(y_test_prediction - y_test)) * 100))\n",
    "\n",
    "    model_data = {\n",
    "        \"w\": w,\n",
    "        \"b\": b,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"num_iterations\": num_iterations,\n",
    "        \"y_train_prediction\": y_train_prediction,\n",
    "        \"y_test_prediction\": y_test_prediction\n",
    "    }\n",
    "    \n",
    "    return model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Combining and making a whole pipeline\n",
    "1. Load the Dataset\n",
    "2. Preprocess the data\n",
    "3. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "x_train_original, y_train_original, x_test_original, y_test_original = load_dataset()\n",
    "\n",
    "# preprocess the data\n",
    "x_train_preprocess, y_train_preprocess, x_test_preprocess, y_test_preprocess = preprocessing(x_train_original, y_train_original, x_test_original, y_test_original)\n",
    "\n",
    "# train the model\n",
    "logistic_regression_model = model(x_train_preprocess, y_train_original, x_test_preprocess, y_test_original, num_iterations = 2000, learning_rate = 0.005, print_cost_requested = True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "df0893f56f349688326838aaeea0de204df53a132722cbd565e54b24a8fec5f6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
